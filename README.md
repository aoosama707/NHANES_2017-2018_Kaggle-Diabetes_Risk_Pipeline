# NHANES 2017–2018 (Kaggle) — Diabetes Risk Pipeline

End-to-end pipeline for predicting **diabetes risk** using the **NHANES 2017–2018** Kaggle tabular dataset.  
Includes data merging/cleaning, baseline **Logistic Regression (LR)** + **XGBoost (XGB)** modeling, **probability calibration**, **clinical utility (Decision Curve Analysis)**, **SHAP interpretability**, and **subgroup fairness / threshold** analyses.

---

## What’s inside

### Key features
- ✅ Load and merge NHANES Kaggle CSVs from `data/` (subfolders allowed)
- ✅ Clean dataset export: `data/nhanes_kaggle_2017_2018_cleaned.csv`
- ✅ Model training:
  - Logistic Regression (baseline)
  - XGBoost (high-performance comparator)
- ✅ Calibration:
  - Isotonic calibration for LR probabilities
- ✅ Evaluation outputs:
  - ROC/PR-AUC, Brier score
  - Confusion matrices (multiple thresholds supported)
  - Calibration curves (overall + by subgroup)
  - Decision Curve Analysis (DCA)
- ✅ Interpretability:
  - SHAP summary + example force plot
- ✅ Fairness / subgroup analysis:
  - Metrics by age/gender/race groups
  - Threshold sweeps and “Recall ≥ target” threshold selection (see note below)
- ✅ Saved artifacts for reuse:
  - Joblib bundles (preprocessing + calibrated LR)
  - XGBoost JSON model

---

## Repository structure

```
.
├── data/                         # Put Kaggle NHANES CSVs here (keep subfolders if any)
├── figures/                      # Plots generated by the notebook
├── models/                       # Saved artifacts (joblib + xgb json)
├── static/                       # Web app static files (if used)
├── templates/                    # Web app HTML templates (if used)
├── app.py                        # Simple web app for inference (loads models/)
├── nhanes_kaggle_pipeline_final.ipynb
└── requirements.txt
```

---

## Quickstart

### 1) Install dependencies
```bash
python -m venv .venv
# Windows: .venv\Scripts\activate
source .venv/bin/activate

pip install -r requirements.txt
```

### 2) Add data
Download the **NHANES 2017–2018 Kaggle** CSV files and place them under:
```
data/
```
You can keep Kaggle’s folder structure; the notebook searches recursively.

### 3) Run the notebook
```bash
jupyter notebook
```
Open:
- `nhanes_kaggle_pipeline_final.ipynb`

Run cells top-to-bottom. The notebook will:
- load and inspect files,
- standardize/rename columns (mapping step),
- create a merged dataset,
- train/evaluate models,
- save figures and model artifacts.

---

## Outputs

### Cleaned dataset
- `data/nhanes_kaggle_2017_2018_cleaned.csv`

### Saved models
Generated under `models/` (after running modeling cells):
- `models/prep_and_lr_cal.joblib`  
  Contains: `imputer`, `scaler`, and calibrated LR object (`lr_cal`)
- `models/lr_tuned.joblib`  
  Same idea, but produced from the tuned LR path (GridSearch + calibration)
- `models/xgb_model.json`  
  XGBoost model saved in JSON format

### Figures
Generated under `figures/` (examples):
- ROC curves: `roc_lr.png`, `roc_xgb.png`
- Confusion matrices: `confusion_matrix_LR.png`, `confusion_matrix_xgb.png`
- Calibration: `Calibration_Curve.png`, `calibration_by_age.png`, `calibration_by_gender.png`, `calibration_by_race.png`
- Decision curves: `dca_lr.png`, `dca_xgb.png`
- SHAP: `shap_summary.png`, `shap_force_example.png`
- EDA: correlation and distribution grids

---

## Running the web app (optional)

If you want a simple UI for inference, make sure you have already generated model files in `models/`, then run:

```bash
python app.py
```

Open (local):
```text
http://127.0.0.1:5000/
```

> If the app expects a specific model filename (e.g., `prep_and_lr_cal.joblib`), run the notebook once to create it.

---

## Notes on fairness + threshold selection

The notebook includes subgroup threshold selection to achieve a target recall (e.g., **Recall ≥ 0.80**).  
If you want a strict evaluation protocol, tune subgroup thresholds on a **validation split** (from training) and only report final results once on the held-out test set.

---

## Reproducibility tips
- Use a fixed `random_state` for splits and models.
- Prefer an sklearn `Pipeline` (imputer/scaler/model) during cross-validation to avoid preprocessing leakage.
- Save artifacts in `models/` for consistent reuse.

---

## Acknowledgements
Data source: **NHANES 2017–2018 (Kaggle)**.

---
